{
 "cells": [
  {
   "cell_type": "code",
   "id": "e9db0b2e-c5c1-44b1-8e35-455356eb59f6",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import m2cgen as m2c\n",
    "\n",
    "# file_path = \"New_Combined_MarchMadness_Data.csv\"\n",
    "# file_path = \"all_march_madness_data.csv\"\n",
    "file_path = \"data_sets/least_variables.csv\"\n",
    "# file_path = \"no_rank_variable.csv\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e666bfd4-253e-4754-9038-552f28fa84e7",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. CREATE TARGET + KEEP ROUND 64 ONLY\n",
    "# ---------------------------------------------------------\n",
    "df['first_round_win'] = df['ROUND'].apply(lambda x: 0 if x == 64 else 1)\n",
    "\n",
    "X_numeric = X.select_dtypes(include='number')\n",
    "\n",
    "df_first_round = df[df['ROUND'] == 64].copy()\n",
    "df_first_round = df_first_round.dropna(subset=['first_round_win'])\n",
    "\n",
    "y = df_first_round['first_round_win']\n",
    "X = df_first_round.drop(columns=['first_round_win', 'ROUND'], errors='ignore')\n",
    "\n",
    "cols_to_drop = [\n",
    "    'CURRENT ROUND',\n",
    "    'team',\n",
    "    'SEED',  # drop duplicates\n",
    "]\n",
    "\n",
    "X = X.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. TRAIN/TEST SPLIT\n",
    "# ---------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.30,\n",
    "    random_state=92,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. FIT RANDOM FOREST\n",
    "# ---------------------------------------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    random_state=92,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "base_acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Base Accuracy (all features):\", base_acc)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. FEATURE IMPORTANCE PLOT\n",
    "# ---------------------------------------------------------\n",
    "importances = rf.feature_importances_\n",
    "feat_df = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feat_df['feature'], feat_df['importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop features sorted by importance:\\n\")\n",
    "print(feat_df)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6. SEARCH BEST NUMBER OF TOP FEATURES\n",
    "# ---------------------------------------------------------\n",
    "best_acc = 0\n",
    "best_k = None\n",
    "best_features = None\n",
    "\n",
    "sorted_features = feat_df['feature'].tolist()\n",
    "\n",
    "for k in range(1, len(sorted_features) + 1):\n",
    "    selected = sorted_features[:k]\n",
    "\n",
    "    X_train_k = X_train[selected]\n",
    "    X_test_k = X_test[selected]\n",
    "\n",
    "    rf_k = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=92,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    rf_k.fit(X_train_k, y_train)\n",
    "    y_pred_k = rf_k.predict(X_test_k)\n",
    "\n",
    "    acc_k = accuracy_score(y_test, y_pred_k)\n",
    "\n",
    "    if acc_k > best_acc:\n",
    "        best_acc = acc_k\n",
    "        best_k = k\n",
    "        best_features = selected\n",
    "\n",
    "print(\"\\n===================================================\")\n",
    "print(f\"BEST ACCURACY FOUND: {best_acc:.4f} using top {best_k} features\")\n",
    "print(\"===================================================\")\n",
    "print(\"Best feature set:\")\n",
    "print(best_features)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 7. PRINT THEIR IMPORTANCES\n",
    "# ---------------------------------------------------------\n",
    "best_imp_df = feat_df[feat_df['feature'].isin(best_features)]\n",
    "\n",
    "print(\"\\nFeature importances for BEST model:\\n\")\n",
    "print(best_imp_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95763f2e-01e8-4951-a3f6-4e0a2272001f",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32b8755d-8ea8-47c6-a411-181b103623d6",
   "metadata": {},
   "source": [
    "# Load data\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create first_round_win: 0 if ROUND == 64, else 1\n",
    "df['first_round_win'] = df['ROUND'].apply(lambda x: 0 if x == 64 else 1)\n",
    "\n",
    "# Keep only the first-round row (ROUND == 64) for each team/year\n",
    "df_first_round = df[df['ROUND'] == 64].copy()\n",
    "\n",
    "# Drop any missing targets\n",
    "df_first_round = df_first_round.dropna(subset=['first_round_win'])\n",
    "\n",
    "# Now target and features\n",
    "y = df_first_round['first_round_win']\n",
    "X = df_first_round.drop(columns=['first_round_win', 'ROUND'], errors='ignore')\n",
    "\n",
    "# --- Create difference features for opponent stats ---\n",
    "opponent_cols = [col for col in X.columns if col.startswith(\"Opp\")]\n",
    "\n",
    "for opp_col in opponent_cols:\n",
    "    # Match the team column by removing 'Opp'\n",
    "    team_col = opp_col.replace(\"Opp\", \"\")\n",
    "    \n",
    "    if team_col in X.columns:\n",
    "        diff_col = team_col + \"_diff\"\n",
    "        X[diff_col] = X[team_col] - X[opp_col]\n",
    "\n",
    "# Drop the original team and opponent columns to avoid redundancy\n",
    "cols_to_drop = opponent_cols + [col for col in X.columns if col in [c.replace(\"Opp\", \"\") for c in opponent_cols]]\n",
    "X = X.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Add additional interaction features\n",
    "X[\"AdjEM_AdjTempo_ratio\"] = X[\"AdjEM\"] / (X[\"AdjTempo\"] + 1e-6)  # avoid divide by zero\n",
    "X[\"Height_Experience_interaction\"] = X[\"AvgHeight\"] * X[\"Experience\"]\n",
    "X[\"Seed_AdjEM_interaction\"] = X[\"SEED\"] * X[\"AdjEM\"]\n",
    "\n",
    "print(\"Features used by the model:\", X.columns.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36c08d39-10b3-4ba6-8726-30533810d168",
   "metadata": {},
   "source": [
    "# TEST AND TRAIN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in X_encoded.columns:\n",
    "    if X_encoded[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_encoded,          \n",
    "    y,          \n",
    "    test_size=0.3,   \n",
    "    random_state=92, \n",
    "    stratify=y      \n",
    ")\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set:\", X_test.shape, y_test.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bea9788-43b3-4e94-be4a-ad567791937b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# --- Train Random Forest without hyperparameter tuning ---\n",
    "        \n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # optional\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_base.predict(X_test)\n",
    "\n",
    "importances = rf_base.feature_importances_\n",
    "feat_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_df['feature'], feat_df['importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69d2cd73-cfd5-4957-80a9-f804ba31a35a",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "986e74de-63f5-4c9a-82fb-98384b3bdd7c",
   "metadata": {},
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    random_state=92\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4. FEATURE IMPORTANCE PLOT\n",
    "# ------------------------------------------------------\n",
    "importances = rf.feature_importances_\n",
    "feat_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": importances\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_df['feature'], feat_df['importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5. LOOP: TEST ACCURACY USING TOP k FEATURES\n",
    "# ------------------------------------------------------\n",
    "accuracies = []\n",
    "\n",
    "for k in range(1, len(feat_df)+1):\n",
    "    top_k_features = feat_df['feature'].iloc[:k]\n",
    "    \n",
    "    # Train RF using only top k variables\n",
    "    rf_k = RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=92\n",
    "    )\n",
    "    rf_k.fit(X_train[top_k_features], y_train)\n",
    "\n",
    "    y_pred_k = rf_k.predict(X_test[top_k_features])\n",
    "    acc_k = accuracy_score(y_test, y_pred_k)\n",
    "    \n",
    "    accuracies.append((k, acc_k))\n",
    "\n",
    "# Find best number of variables\n",
    "best_k, best_acc = max(accuracies, key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n-------------------------------\")\n",
    "print(f\"Best accuracy achieved with k = {best_k} variables\")\n",
    "print(f\"Accuracy: {best_acc:.4f}\")\n",
    "print(\"-------------------------------\\n\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6. PRINT BEST FEATURES + IMPORTANCE VALUES\n",
    "# ------------------------------------------------------\n",
    "best_features = feat_df.iloc[:best_k]\n",
    "print(\"Top features producing highest accuracy:\\n\")\n",
    "print(best_features)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a07f14bc-5a44-4d66-ac0e-8da787f78154",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Train Random Forest without hyperparameter tuning ---\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # optional\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_base.predict(X_test)\n",
    "\n",
    "# --- Evaluate ---\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Feature Importance ---\n",
    "importances = rf_base.feature_importances_\n",
    "features = X_train.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Feature Importances (Random Forest)\")\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), [features[i] for i in indices], rotation=45)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5239d0c-ed24-4b8e-bd39-0a85114b1cea",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Step 1: Determine feature ranking ---\n",
    "importances = rf_base.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]  # descending order\n",
    "features = X_train.columns\n",
    "\n",
    "# --- Step 2: Track best accuracy for subsets (your existing loop) ---\n",
    "best_acc = 0\n",
    "best_n = 0\n",
    "best_features = []\n",
    "accuracy_results = []\n",
    "\n",
    "rf_params = {\n",
    "    'random_state': 42,\n",
    "    'class_weight': 'balanced',\n",
    "}\n",
    "\n",
    "for n in range(1, len(features) + 1):\n",
    "    top_features = features[indices[:n]]\n",
    "    \n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    rf_model.fit(X_train[top_features], y_train)\n",
    "    \n",
    "    y_pred = rf_model.predict(X_test[top_features])\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracy_results.append((n, acc))\n",
    "    \n",
    "    print(f\"Top {n} features â†’ Accuracy = {acc:.4f}\")\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_n = n\n",
    "        best_features = top_features\n",
    "\n",
    "print(\"\\n=== Best Result ===\")\n",
    "print(f\"Number of top features: {best_n}\")\n",
    "print(f\"Features: {list(best_features)}\")\n",
    "print(f\"Highest accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# --- Step 3: Build final model using top 7 features ---\n",
    "top_7_features = features[indices[:7]]\n",
    "final_model = RandomForestClassifier(**rf_params)\n",
    "final_model.fit(X_train[top_7_features], y_train)\n",
    "\n",
    "# --- Step 4: Evaluate final model ---\n",
    "y_pred_final = final_model.predict(X_test[top_7_features])\n",
    "print(\"\\n=== Final Model Evaluation (Top 7 Features) ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74376976-7b1d-41f0-9631-70e87d90f850",
   "metadata": {},
   "source": [
    "# Export the trained RF to JS code\n",
    "js_code = m2c.export_to_javascript(final_model)\n",
    "\n",
    "# Save it to a JS file\n",
    "with open(\"model.js\", \"w\") as f:\n",
    "    f.write(js_code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d16b9200-1ef4-48f8-bef6-d99c3d3e9045",
   "metadata": {},
   "source": [
    "year_to_test = 2024\n",
    "X_year = X[df['year'] == year_to_test]\n",
    "y_year = y[df['year'] == year_to_test]\n",
    "\n",
    "# Select top 7 features\n",
    "X_year_top7 = X_year[top_7_features]\n",
    "\n",
    "# Predict\n",
    "y_pred_year = final_model.predict(X_year_top7)\n",
    "y_prob_year = final_model.predict_proba(X_year_top7)  # optional: probabilities\n",
    "\n",
    "# Create a results DataFrame\n",
    "results_year = X_year_top7.copy()\n",
    "results_year['Actual'] = y_year\n",
    "results_year['Predicted'] = y_pred_year\n",
    "results_year['Prob_A'] = y_prob_year[:,0]   # probability team A wins\n",
    "results_year['Prob_B'] = y_prob_year[:,1]   # probability team B wins\n",
    "\n",
    "# Print the results\n",
    "print(results_year)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e04ea19-9cc6-491d-bc4b-3ba170653925",
   "metadata": {},
   "source": [
    "df_tourney = df[(df['year'] == 2019) & (df['ROUND'] < 64)]\n",
    "print(df_tourney)\n",
    "\n",
    "year_to_test = 2019\n",
    "X_year = X[df['year'] == year_to_test]\n",
    "y_year = y[df['year'] == year_to_test]\n",
    "\n",
    "# --- Select top 7 features ---\n",
    "X_year_top7 = X_year[top_7_features]\n",
    "\n",
    "# --- Predict ---\n",
    "y_pred_year = final_model.predict(X_year_top7)\n",
    "y_prob_year = final_model.predict_proba(X_year_top7)  # gives probabilities for each class\n",
    "\n",
    "# --- Evaluate ---\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "print(f\"\\n=== Final Model Evaluation on {year_to_test} ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_year, y_pred_year):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_year, y_pred_year))\n",
    "\n",
    "# --- Optional: Inspect individual predictions ---\n",
    "results_year = X_year_top7.copy()\n",
    "results_year['Actual'] = y_year\n",
    "results_year['Predicted'] = y_pred_year\n",
    "results_year['Prob_A'] = y_prob_year[:,0]\n",
    "results_year['Prob_B'] = y_prob_year[:,1]\n",
    "\n",
    "print(results_year)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbd337f4-f604-48c3-95ef-3c0a6437b9c0",
   "metadata": {},
   "source": [
    "print(top_features)\n",
    "print(X_train.columns.tolist())\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Define base Random Forest ---\n",
    "# Holding number of estimators fixed\n",
    "rf_base = RandomForestClassifier(\n",
    "    n_estimators=100,          # <-- held constant\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# --- Define hyperparameter grid ---\n",
    "# Explore depth from 2 to 30 (step = 2)\n",
    "param_grid = {\n",
    "    'max_depth': list(range(2, 32, 2)),\n",
    "    'min_samples_split': [60],\n",
    "    'min_samples_leaf': [7]\n",
    "}\n",
    "\n",
    "# --- Grid Search ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# --- Use only top features ---\n",
    "top_features = [\n",
    "    'AdjEM_AdjTempo_ratio', 'AdjEM', 'SEED',\n",
    "    'Seed_AdjEM_interaction', 'FG2Pct_diff', 'ORPct', 'TOPct'\n",
    "]\n",
    "\n",
    "grid_search.fit(X_train[top_features], y_train)\n",
    "\n",
    "# --- Best model ---\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7c844f8-ec07-45a9-8f4b-d613cc14b050",
   "metadata": {},
   "source": [
    "p_obs = 86.76 / 100\n",
    "\n",
    "# Hypothesized proportion\n",
    "p0 = 78.125 / 100\n",
    "\n",
    "# Sample size\n",
    "n = 100  # replace with your actual sample size\n",
    "\n",
    "# Number of \"successes\" in the sample\n",
    "count = int(p_obs * n)\n",
    "\n",
    "# Perform one-proportion z-test\n",
    "stat, pval = proportions_ztest(count, n, value=p0)\n",
    "print(f\"Z-statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {pval:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "948c71fc-bd56-45a1-8cc8-c64f4a57645d",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Define base Random Forest ---\n",
    "rf_base = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# --- Define hyperparameter grid ---\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [60],\n",
    "    'min_samples_leaf': [7]\n",
    "}\n",
    "\n",
    "# # --- Run Grid Search with 3-fold cross-validation ---\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Use only top features\n",
    "top_features = ['AdjEM_AdjTempo_ratio', 'AdjEM', 'SEED', \n",
    "                'Seed_AdjEM_interaction', 'FG2Pct_diff', 'ORPct', 'TOPct']\n",
    "\n",
    "grid_search.fit(X_train[top_features], y_train)\n",
    "\n",
    "# --- Best model ---\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "y_pred = best_rf.predict(X_test[top_features])\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Feature Importance ---\n",
    "importances = best_rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"Feature Importances (Tuned Random Forest)\")\n",
    "plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "plt.xticks(range(len(importances)), [top_features[i] for i in indices], rotation=45)\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()\n",
    "\n",
    "# --- Visualize accuracy as a function of certain hyperparameters ---\n",
    "# Example: n_estimators vs mean CV accuracy\n",
    "results = grid_search.cv_results_\n",
    "mean_test_scores = results['mean_test_score']\n",
    "n_estimators_list = param_grid['min_samples_leaf']\n",
    "\n",
    "# Aggregate scores by n_estimators\n",
    "avg_scores = []\n",
    "for n in n_estimators_list:\n",
    "    idx = [i for i, params in enumerate(results['params']) if params['min_samples_leaf'] == n]\n",
    "    avg_scores.append(np.mean(mean_test_scores[idx]))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_estimators_list, avg_scores, marker='o')\n",
    "plt.title(\"Mean CV Accuracy vs min_samples_leaf\")\n",
    "plt.xlabel(\"param_min_samples_leaf\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef777ea3-84f5-4685-8e27-b78fe50b6530",
   "metadata": {},
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Function to plot accuracy vs parameter\n",
    "def plot_param_vs_accuracy(param_name):\n",
    "    param_values = results[param_name]\n",
    "    mean_scores = results['mean_test_score']\n",
    "    \n",
    "    # For discrete parameters, aggregate by value\n",
    "    if param_values.dtype == object or len(np.unique(param_values)) < 10:\n",
    "        agg_scores = results.groupby(param_name)['mean_test_score'].mean()\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(agg_scores.index, agg_scores.values, marker='o')\n",
    "    else:\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.scatter(param_values, mean_scores)\n",
    "    \n",
    "    plt.title(f\"Mean CV Accuracy vs {param_name}\")\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Mean CV Accuracy\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each hyperparameter\n",
    "for param in ['param_n_estimators', 'param_max_depth', \n",
    "              'param_min_samples_split', 'param_min_samples_leaf']:\n",
    "    plot_param_vs_accuracy(param)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ad2bd1b1-a0e2-4bfd-81ac-c7fc8aa185c6",
   "metadata": {},
   "source": [
    "n_features, accs = zip(*accuracy_results)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(n_features, accs, marker='o')\n",
    "plt.xlabel(\"Number of Top Features\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of Top Features\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0bf0d97-ace2-4619-a6bc-b605082ef72a",
   "metadata": {},
   "source": [
    "n_correct = int(best_acc * len(y_test))  # number of correct predictions\n",
    "n_total = len(y_test)                    # total predictions\n",
    "p_null = .75                            # null hypothesis (random guessing)\n",
    "stat, p_value = proportions_ztest(count=n_correct, nobs=n_total, value=p_null, alternative='larger')\n",
    "print(f\"\\nProportion test against 0.5:\")\n",
    "print(f\"Z-statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "369e0689-835b-4822-af48-cb9cbcd6f39e",
   "metadata": {},
   "source": [
    "# Predict first-round winner based on seed\n",
    "# Assuming smaller seed number = stronger team\n",
    "y_pred_seed = (X_test['SEED'] <= 8).astype(int)  # Example: seeds 1-8 are predicted to win\n",
    "\n",
    "# If your target is 1 = team won first round, adjust logic accordingly:\n",
    "# For a more general approach, predict the higher seed in each matchup:\n",
    "y_pred_seed = (X_test['SEED'] <= X_test['SEED'].median()).astype(int)\n",
    "\n",
    "# Compute accuracy of seed-based prediction\n",
    "baseline_acc = accuracy_score(y_test, y_pred_seed)\n",
    "print(f\"Seed-based baseline accuracy: {baseline_acc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56a2769c-b7c5-4c8b-a42d-0b5e53d3d06b",
   "metadata": {},
   "source": [
    "n_correct = int(best_acc * len(y_test))  # number of correct predictions\n",
    "n_total = len(y_test)                    # total predictions\n",
    "p_null = 0.78125                            # null hypothesis using just seed\n",
    "\n",
    "stat, p_value = proportions_ztest(count=n_correct, nobs=n_total, value=p_null, alternative='larger')\n",
    "print(f\"\\nProportion test against 0.5:\")\n",
    "print(f\"Z-statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "755f88e7-4199-4248-935b-371e07651c4b",
   "metadata": {},
   "source": [
    "df['predicted'] = final_model.predict(X[best_features])\n",
    "df['correct'] = df['predicted'] == df['first_round_win']\n",
    "wrong_cases = df[df['correct'] == False]\n",
    "print(wrong_cases.head())\n",
    "print(f\"Total wrong: {len(wrong_cases)} out of {len(df)}\")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(df['first_round_win'], df['predicted']))\n",
    "\n",
    "year_to_check = 2019\n",
    "\n",
    "# Filter for that year\n",
    "df_year = df[df['year'] == year_to_check]\n",
    "\n",
    "# Find wrong predictions in that year\n",
    "wrong_year = df_year[df_year['predicted'] != df_year['first_round_win']]\n",
    "\n",
    "# Show the first few wrong cases\n",
    "print(wrong_year.head())\n",
    "\n",
    "# Total wrong in that year\n",
    "print(f\"Total wrong in {year_to_check}: {len(wrong_year)} out of {len(df_year)}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
